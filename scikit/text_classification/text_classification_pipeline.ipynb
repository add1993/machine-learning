{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8019118428040362\n"
     ]
    }
   ],
   "source": [
    "# Author : Ayush\n",
    "\n",
    "# Downloading the dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "stemmer = EnglishStemmer(ignore_stopwords=True)\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "custom_stop_words=['english', 'slack', 'thanks', 'welcome']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(custom_stop_words)\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words=stop_words,tokenizer=LemmaTokenizer(),lowercase = True, token_pattern=r'\\b[^\\d\\W]+\\b')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print(\"Accuracy = \"+str(np.mean(predicted == twenty_test.target)))\n",
    "\n",
    "# save the model to disk\n",
    "#filename = 'finalized_model.sav'\n",
    "#joblib.dump(text_clf, filename)\n",
    "\n",
    "pkl_filename = \"pipeline_model.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(text_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 1 0 1 1 1 0 1 1 4 0 1]\n",
      " [0 1 0 1 0 1 0 0 0 1 1 0 0 1 1]]\n",
      "{'my': 10, 'the': 12, 'in': 8, '123abc': 0, 'an': 2, 'bigger': 4, 'dog': 6, 'win': 14, 'grin': 7, 'stun': 11, '345': 1, 'this': 13, 'be': 3, 'lazi': 9, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "# This is for testing out stemming and CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import pandas as pd\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "df = pd.DataFrame(['my the the the in 123abc an the bigger dog win grinning stunning', 'my 345 this be lazy cat winning'])\n",
    "value_list = [row[0] for row in df.itertuples(index=False, name=None)]\n",
    "my_additional_stop_words=['my', 'big', 'english']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_words, token_pattern=r'\\b[^\\d\\W]+\\b', analyzer=stemmed_words)\n",
    "\n",
    "x_train = cv.fit_transform(value_list)\n",
    "print(x_train.toarray())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
